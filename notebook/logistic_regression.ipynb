{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import logging\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary paths\n",
    "NOTEBOOK_PATH = pathlib.Path().resolve()\n",
    "LOG_DIRECTORY = NOTEBOOK_PATH.parent / \"log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will <b>build a logistic regression model from scratch</b> and use it to <b>classify the iris dataset</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "- Load the iris dataset from sklearn, use data for the classes 0 and 1 only for this example\n",
    "- Split chosen data into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "## testing \n",
    "# X = np.append(iris.data[:3], iris.data[50:53], axis=0)\n",
    "# y = np.append(iris.target[:3], iris.target[50:53], axis=0)[:, np.newaxis]\n",
    "X = iris.data[:100]\n",
    "y = iris.target[:100][:, np.newaxis]\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the sigmoid function\n",
    "- Recall a sigmoid function is defined by:<br> \n",
    "$$S(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy array): the linear combination of features\n",
    "    \n",
    "    Returns:\n",
    "        return a probability (0-1)\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test sigmoid\n",
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the predict function\n",
    "- The predicted probabiliy of a logistic regression model is given by:<br>\n",
    "$$h_{\\beta}(x) = S(\\beta \\cdot x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat function\n",
    "def y_hat(beta, X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        beta (numpy array): weight vector in linear regression, dimension 1 x n\n",
    "        X (numpy array): training data, dimension m x n\n",
    "        \n",
    "    Returns:\n",
    "        return predicted probabilities, dimension 1 x n\n",
    "    \"\"\"\n",
    "    return sigmoid(np.dot(X, beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the first two rows of training data for testing\n",
    "beta = np.zeros((X.shape[1], 1))\n",
    "Xtry = Xtrain[0:2]\n",
    "ytry = ytrain[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5],\n",
       "       [0.5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test yhat with trial data\n",
    "yhat = y_hat(beta, Xtry)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the cost function\n",
    "- Suppose we have $m$ rows of training data and $n$ features\n",
    "- Recall the cost function of logistic regression is given by:<br> \n",
    "$$J(\\beta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y_i log(\\hat{y_i})+(1-y_i)log(1-\\hat{y_i})]$$\n",
    "where $y_i$ and $\\hat{y_i}$ are the true values and predicted values of i-th row of training data respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function per row of data\n",
    "def cost_per_row(y, yhat):\n",
    "    if y == 1:\n",
    "        return np.multiply(y, np.log(yhat)) \n",
    "    else:\n",
    "        return np.multiply(1-y, np.log(1 - yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6931471805599453"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test cost_per_row\n",
    "cost_per_row(0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total cost\n",
    "def cost(Y, Yhat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Y (numpy array): labels of training data, dimension 1 x m\n",
    "        Yhat (numpy array): predictied values of training data, dimension 1 x m\n",
    "        \n",
    "    Returns:\n",
    "        return the cost of beta\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    df1 = pd.DataFrame(data={'y': Y.ravel(), 'yhat': Yhat.ravel()})\n",
    "    diff = df1.apply(lambda row: cost_per_row(row['y'], row['yhat']), axis=1).to_numpy().reshape(-1, 1)\n",
    "    return -np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test cost\n",
    "cost(ytry, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the gradient\n",
    "- We will use gradient descent for optimizing the cost function above.\n",
    "- Write a function to compute gradient for updating the weight $\\beta$ when feeding each row of training data\n",
    "- Recall in sec 2.1 the weight updating formula is given by:<br>\n",
    "$$\\beta_{k+1} = \\beta_{k} - \\alpha \\frac{\\partial J}{\\partial \\beta}(\\beta_k)$$\n",
    "where $\\alpha$ is a fixed learning rate, $k$ is the number of iterations/epochs.\n",
    "- The gradient is<br><br>\n",
    "$$\\frac{\\partial J}{\\partial \\beta} = (\\frac{\\partial J}{\\partial \\beta_0}, \\frac{\\partial J}{\\partial \\beta_1},..., \\frac{\\partial J}{\\partial \\beta_n})$$\n",
    "- We only need to compute $$\\frac{\\partial J}{\\partial \\beta_j} = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y_i}-y_i)x_{ij}$$ for each $j = 0, 1, 2,... ,n$.\n",
    "- Derivation of the gradient formula of logistic regression can be found at p.18 of:<br> \n",
    "https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient function\n",
    "def gradient(beta, X, y, yhat):\n",
    "    # initialize the gradient as a zero vector\n",
    "    grad = np.zeros(beta.shape)\n",
    "    \n",
    "    # compute delta J/delta beta for each j\n",
    "    for j in range(X.shape[1]):\n",
    "        first = np.multiply(yhat - y, X[:, j][: ,np.newaxis])\n",
    "        grad[j] = np.mean(first)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.  ],\n",
       "       [-1.45],\n",
       "       [-2.05],\n",
       "       [-0.7 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test gradient\n",
    "gradient(beta, Xtry, ytry, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the gradient descent function\n",
    "- Initialize beta as a zero vector, for each epoch, feed the whole train set to calculate: \n",
    "    1. yhat (prediction of training data using existing beta)\n",
    "    2. gradient vector\n",
    "    3. update beta using formula in 2.3\n",
    "    4. cost/loss of existing prediction\n",
    "- Remember to add a constant column (e.g. a column with all 1) to Xtrain that represents the <b>intercept</b> of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 296 µs, sys: 556 µs, total: 852 µs\n",
      "Wall time: 705 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename=LOG_DIRECTORY / \"train.log\", encoding='utf-8', level=logging.DEBUG, force=True)\n",
    "\n",
    "def train_grad_desecnt(X, y, epochs=1000, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        epochs (int): no. of iterations allowed for optimization\n",
    "        alpha (floats): learning rate\n",
    "    \n",
    "    Returns:\n",
    "        beta (numpy array): the final weight\n",
    "        loss (float): the final cost\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    beta = np.zeros((n+1, 1))    # +1 accounts for constant coefficient beta_0\n",
    "    X = np.hstack((np.ones((m, 1)), X))    # add a constant column for constant coefficient\n",
    "    losses = []\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        yhat = y_hat(beta, X)\n",
    "        grad = gradient(beta, X, y, yhat)\n",
    "                        \n",
    "        # update beta and loss\n",
    "        beta -= alpha*grad\n",
    "        loss = cost(y, yhat)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # log result of each epoch\n",
    "        logging.info(\"*\"*50)\n",
    "        logging.info(f'epoch = {epoch}')\n",
    "        logging.info(f'beta = {beta}')\n",
    "        logging.info(f'grad = {grad}')\n",
    "        #logging.info(f'yhat = {yhat[:50]}')\n",
    "        #logging.info(f'y = {y[:50]}')\n",
    "        logging.info(f'loss = {loss}')\n",
    "        \n",
    "    return beta, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_final =  [[-0.35313805]\n",
      " [-0.46177555]\n",
      " [-2.09037366]\n",
      " [ 3.01293972]\n",
      " [ 1.33824509]]\n",
      "loss =  0.0077567443047278925\n",
      "CPU times: user 1.92 s, sys: 21.9 ms, total: 1.94 s\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fit training set into train_gard_descent function\n",
    "beta_final, losses = train_grad_desecnt(Xtrain, ytrain)\n",
    "print('beta_final = ', beta_final)\n",
    "print('loss = ', losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the predict function for new data\n",
    "- The function should use beta calculated in gradient descent and Xtest to return predicitons and predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use trained beta to predict new data\n",
    "def predict(beta, X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (numpy array): feature to predict\n",
    "        \n",
    "    Returns:\n",
    "        y (numpy array): predict results (0 or 1)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    X = np.hstack((np.ones((m, 1)), X))    # add constant column to X for intercept\n",
    "    pred_proba = sigmoid(np.dot(X, beta)).ravel()\n",
    "    print(pred_proba.shape)\n",
    "    pred = np.array([round(x) for x in pred_proba])\n",
    "    return pred, pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find accuracy of model on test data\n",
    "from sklearn.metrics import accuracy_score\n",
    "ypred, ypred_proba = predict(beta_final, Xtest)\n",
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.95799570e-01, 9.98890025e-01, 9.97852823e-01, 4.20178672e-03,\n",
       "       9.97741322e-01, 9.98418422e-01, 5.11400991e-02, 9.98265663e-01,\n",
       "       9.93678890e-01, 9.96196031e-01, 7.46767781e-03, 9.74623091e-01,\n",
       "       5.47415895e-04, 6.81204464e-03, 9.95258850e-01, 6.50663366e-03,\n",
       "       3.03637496e-03, 9.99510463e-01, 1.12998599e-02, 1.20260491e-03,\n",
       "       4.47302413e-03, 2.15560638e-02, 1.03092611e-02, 4.60555138e-03,\n",
       "       9.78354426e-01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see if the predicted probabilities look normal \n",
    "ypred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is 1.0 and the prediced probabilities look normal, great! Note that this implementation is only a simple version, it may not work well in more complex dataset. More details like <b>regularization</b> and <b>weighted cost function</b> (for imbalanced data) can be added to advance the model. Also, data preprocessing like <b>feature scaling</b> may be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
